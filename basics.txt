Set environment variable $OV_DATA_BASE

folders:
- raw_data: contains raw segmentation data (images, labels) as nifti (or dcm)

- preprocessed: contains preprocessed numpy data. Each dataset can be preprocessed in different ways (e.g. half and full resolution)
	splits contain information on how the scans are split in training and validation, preprocessed_parameters remeber how the raw data was converted
	fingerprints contain information on the scans and their past

- plots/predictions: what they say they are, stored as .png or .nii.gz (compressed nifit) for each model

- trained_models: each raw data has a subfolder with models. These model folders contain information prediction metrics, model parameters, network weights and training curves

Models:
- contains all infromation the uniquely defines the algorithm: preprocessing, augmentation, network, postprocessing, prediction, training and data
- each of these are seperate objects
- a model is created by assigning 
	1) val_fold: which fold/split is used for this model
 	2) data_name: telling what the name of the raw data was
	3) model_name: the name of the folders in trained_models, plots, predictions
	4) model_parameters: long dict of all information that is used to build up the model
	5) preprpocessed_name (optional): tells the model which of the preprocessed folders is the one to choose from the folder $OV_DATA_BASE/preprocessed/data_name, default: pick folder if only one exists, else error

- a model can be trained by calling model.training.train()
- a model can be evaluated by calling model.eval_validataion_set()
- a data_tpl can be predicted by calling model[data_tpl]

Data:
- raw data is stored as nifit or dcm in $OV_DATA_BASE/raw_data(/data_name)
- perprocessed data is stored as npy files in $OV_DATA_BASE/preprocessed/data_name/preprocessed_name
- the data object typically has val_ds, trn_ds (datasets of validation, training data) and val_dl, trn_dl (dataloaders of training and validation data)
- data_tpls are dicts containing npy arrays and patient information (scan date, BL or FU, id etc.)
- batches are dicts containing the tensors used for training


Typical workflow:
1) copy raw data to $OV_DATA_BASE/raw_data
2) preprocessed raw data to npy files by running a preprocessing script/calling the right functions of a preprocessing object
3) create a model by choosing the name and create the set of parameters etc. (see above)
4) train the model by running model.training.tain()
5) eval the model by running model.eval_validataion_set() and model.eval_training_set()